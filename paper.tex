\documentclass{article}
\usepackage{amsmath}
\usepackage{cite}
\usepackage[dvips]{graphicx}
\usepackage{listings}
\bibliographystyle{plain}

\begin{document}
\title{A Survey of Modern A-Life and Synthetic Evolution}
\author{Vaibhav Mallya}
\maketitle
\section{Abstract}

Artificial life is one of the most sought-after yet most-unfulfilled dreams of the computing era.
What makes the search for A-life, and open-ended evolution in particular, such an important part of the search for a general theory of complex adaptive systems?
In this paper, we dissect the well-known A-life simulation Tierra, and try determine where it failed. We pose the question: what might be necessary for a good solution to the problem to succeed?

\section{Introduction}
As the computer sciences have advanced, we have seen an increase in numerous predictive and adaptive technologies. Regardless, the hard A-life problem of open-ended evolution has remained relatively unfazed since most of these advances have been in information retrieval, data mining, and machine learning.

The study of complex adaptive systems is intimately linked to the search for artificial life in no small part because biological ecosystems are a classic case study in chaos, adaptation, and nonlinearity. One of the problems in studying real-world cas (indeed, studying any chaotic system) is that making any accurate prediction beyond the short or medium term is impossible, simply because the full state of the system cannot be totally determined. 
Small errors in measurement snowball rapidly, and cause any predictive model to quickly spiral to inaccuracy. If we knew we could simulate a cas in its entirety \emph{in silico} - that is, without any error in determination of state - we could use it to validate potential unifying theories with a much higher degree of certainity.

A-life is important precisely because we can have total knowledge of the system at any given point of time. By observing the environment in a completely controlled system, we can eliminate the uncertainities in measurement that frustrate so many real-world models. 

Additionally, we can exploit the fact that since we have total control over the simulated environment, we retain the ability to perturb the simulation in a way that allows us to test specific facets of the theory. This is obviously non-trivial to accomplish in, for example financial systems.

\section{Definitions}

s that simulate artificial life have to choose precisely where to draw boundaries between what to give as primitives and what should be emerged. We don't even know what boundaries are relevant. There isn't even a single universally accepted definition of life itself! We propose yet another definition for life:

Realing A key component in any potential unified theory of complex adaptive systems, the twin disciplines have experienced a long winter of disillusionment and lack of visibility since the glory days of the 1960s. However, the field has not been completely dormant, and incremental progress has been made on numerous fronts, including synthetic evolution, cognitive architecture, and other areas.

\section{Description of Tierra}
Tierra was an open-ended digital experiment conducted by ecologist Thomas Ray. It was derived from a modestly popular two-person programmer's game created during the 1980s called Core Wars. The premise of Core Wars is as follows:

\begin{itemize}
\item The playing field is a two-dimensional grid that represents a virtual machine's memory space
\item Every square of the grid contains a Redcode instruction
\item Players write a "warrior" (programs) in Redcode
\item The grid is randomly populated with each player's warrior at once to begin the game
\item The simulation then produces automatically and completely deterministically
\item Warrior-programs are deterministically executed one after another, with players alternating 
\item The player whose warriors dominate the virtual memory space after some number of cycles wins.
\end{itemize}

Redcode is a simple abstract RISC assembly language:

\begin{itemize}
\item Every instruction is a three-tuple of opcode, source address, destination address
\item Minimal opcode set (10 total)
\item In addition to add/subtract, notable opcodes include copy and move
\item Impossible to create an invalid instruction in the underlying numeric representation
\item Instructions are indivisible - no partial overwriting or copying
\end{itemize}

	Although very basic, there is a surprising amount of complexity that can emerge very quickly when playing Core Wars. For instance, as a result of the instruction set and the rules of the game, polymorphism is the rule rather than the exception. Additionally, there have even been several implementations of the genetic algorithm to breed optimize Core Wars warriors. What Thomas Ray did with Tierra was the following: 

\begin{itemize}
\item Instead of being perfect, cloning operations randomly mutate the genotype
\item A background "reaver" killed off programs that were deemed too old
\item No players and no explicit notion of winning or losing
\end{itemize}

The initial grid was populated with a very simple 80-byte large program that just duplicated itself. After a few thousand iterations, the small copy mutation rate caused small variants to appear that were largely identical.
However, after several more generations, drastically different programs "evolved", including parasitic programs that were only 45 bytes long, extremely shorter programs that were parasites of the parasites, and so forth.
Although Tierra's general approach may be promising, Tierra creatures themselves were found to have low overall complexity - the curve of interesting entities flattens out quickly with the provided parameter set. Scaling in particular was a major issue.



Simulations have been either behavorial or structural, a

There are a few common definitions, ranging from the extremely physical
We need spatial locality (?)

\begin{itemize}
\item Adaptation - Continue to function within a range of circumstances
\item Energy conversion - Convert energy from one form to another
\end{itemize}
	
When the Tierra results were released, there was a flurry of press coverage proclaiming the radical second coming of artificial life - and indeed, the behavior exhibited by the modified Core Wars engine ie really quite fascinating. Upon closer inspection however, the creatures produced by Tierra at any reasonable (at the time) scale seemed to have a fairly limited information-theoretic complexity, as demonstrated by  Russel Standish. [were standish's methods experimental or information-theoretical? has there been any information-theoretical work done on this?]


Will having three dimensions in a simulation be necessary? For a structural simulation of life, obviously. But for a behaviorial simulation, perhaps not - we can find abundant diversity in the numbers of bacteria, protozoa, and other microorganisms, and we can say that these coexist on an approximately two-dimensional plane - right? Well, we don't know. But it's a point worth considering.

One thing to note in many of these simulations is that there is an *explicit* way to reproduce or breed. Would it be fruitful to devise a system in which the breeding process itself was "emerged"? This would mean a much lower-level simulation of "stuff" since we'd presumably have to simulate some lower level physics and stuff. Also, we could emerge a more novel set of breeding methods than we currently know about. However, this may not even be necessary.

Darwin@home (hereafter referred to as DH) is a much higher-level take on A-Life. Instead of abstract programs, we deal directly with "creatures" formed via a particular physics simulation known as Tensegrity






 The field of cas - complex adaptive systems


For any theory about tangible phenemena to be developed or advanced, we need to have information about the system(s). We claim that, especially when dealing with the kind of chaotic and intericate systems that characterize cas, we cannot effectively iterate on a theory that attempts to predict the future state of a system unless we have a near-total understanding of the current state of the system. Since we are attempting to develop a theory, we will have to go through the process

Claim:
Good awareness of the state of the system
The ability to rapidly learn from predictions and iterate on the theory itself
The ability to poke the system in a specific way to test some particular component of the theory


As far as our best theories in meterology [2] and economics [3] fail (sometimes spectacularly) past the short term.

[1] http://science.nasa.gov/headlines/y2004/06may_lunarranging.htm
[2] http://freakonomics.blogs.nytimes.com/2008/04/21/how-valid-are-tv-weather-forecasts/
[3] http://www.wired.com/techbiz/it/magazine/17-03/wp_quant?currentPage=1

\bibliography{paper_cite}
\end{document}
